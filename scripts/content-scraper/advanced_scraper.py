#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…Ø­ØªÙˆØ§ Ø¨Ø§ Selenium Ø¨Ø±Ø§ÛŒ Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ JavaScript-heavy
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import time
import json
from pathlib import Path
from content_scraper import ContentScraper


class AdvancedScraper(ContentScraper):
    """Ø§Ø³Ú©Ø±Ù¾Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Selenium Ø¨Ø±Ø§ÛŒ Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ JavaScript-heavy"""
    
    def __init__(self, output_dir: str = "scraped_content", headless: bool = True):
        super().__init__(output_dir)
        self.headless = headless
        self.driver = None
        self.setup_driver()
    
    def setup_driver(self):
        """ØªÙ†Ø¸ÛŒÙ… ChromeDriver"""
        chrome_options = Options()
        if self.headless:
            chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        chrome_options.add_argument('--lang=fa-IR')
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.set_page_load_timeout(30)
        except Exception as e:
            print(f"âš ï¸  Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ ChromeDriver: {e}")
            print("ğŸ’¡ Ù„Ø·ÙØ§Ù‹ ChromeDriver Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø§Ø² content_scraper.py Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯")
            self.driver = None
    
    def scrape_page_selenium(self, url: str) -> Optional[Dict]:
        """Ø§Ø³Ú©Ø±Ù¾ ØµÙØ­Ù‡ Ø¨Ø§ Selenium"""
        if not self.driver:
            return None
        
        try:
            print(f"Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³Ú©Ø±Ù¾ (Selenium): {url}")
            self.driver.get(url)
            
            # Ù…Ù†ØªØ¸Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ lazy-loaded
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            # Ø¯Ø±ÛŒØ§ÙØª HTML
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ØªØ¯Ù‡Ø§ÛŒ ÙˆØ§Ù„Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬
            meta_data = self.extract_meta_tags(soup)
            content = self.extract_content(soup)
            
            # Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ±
            downloaded_images = []
            for img_info in content['images'][:10]:
                img_path = self.download_image(img_info['url'], url)
                if img_path:
                    downloaded_images.append({
                        'path': img_path,
                        'alt': img_info['alt'],
                        'title': img_info['title']
                    })
            
            title = meta_data['title'] or (content['headings'][0]['text'] if content['headings'] else 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')
            slug = self.create_slug(title)
            full_text = ' '.join([p for p in content['paragraphs']])
            
            scraped_data = {
                'id': hashlib.md5(url.encode()).hexdigest()[:12],
                'url': url,
                'slug': slug,
                'title': title,
                'meta_description': meta_data['description'],
                'meta_keywords': meta_data['keywords'],
                'content': full_text,
                'excerpt': full_text[:300] + '...' if len(full_text) > 300 else full_text,
                'headings': content['headings'],
                'images': downloaded_images,
                'scraped_at': datetime.now().isoformat(),
                'source': urlparse(url).netloc,
            }
            
            return scraped_data
            
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±Ù¾ {url}: {e}")
            return None
    
    def __del__(self):
        """Ø¨Ø³ØªÙ† driver Ù‡Ù†Ú¯Ø§Ù… Ø®Ø±ÙˆØ¬"""
        if self.driver:
            self.driver.quit()


if __name__ == "__main__":
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Selenium Ø¨Ø±Ø§ÛŒ Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ
    scraper = AdvancedScraper(headless=True)
    scraper.run()

