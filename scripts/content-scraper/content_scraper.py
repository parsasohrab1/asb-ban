#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø§Ø³Ø¨ Ø§Ø² Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
Ø§ÛŒÙ† Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù…Ø­ØªÙˆØ§ØŒ ØªØµØ§ÙˆÛŒØ± Ùˆ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ SEO Ø´Ø¯Ù‡ Ø±Ø§ Ø§Ø² Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
"""

import os
import re
import json
import time
import requests
from urllib.parse import urljoin, urlparse
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
import hashlib
from PIL import Image
import io

class ContentScraper:
    def __init__(self, output_dir: str = "scraped_content"):
        self.output_dir = Path(output_dir)
        self.images_dir = self.output_dir / "images"
        self.data_dir = self.output_dir / "data"
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§
        self.images_dir.mkdir(parents=True, exist_ok=True)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # User-Agent Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨Ù„Ø§Ú© Ø´Ø¯Ù†
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fa-IR,fa;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # Ù„ÛŒØ³Øª Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø§Ø³Ø¨
        self.target_sites = [
            {
                'name': 'Ø§Ø³Ø¨ Ø§ÛŒØ±Ø§Ù†',
                'base_url': 'https://www.asbiran.com',
                'search_paths': ['/articles', '/blog', '/news'],
                'keywords': ['Ø§Ø³Ø¨', 'Ø³ÙˆØ§Ø±Ú©Ø§Ø±ÛŒ', 'Ù…Ø³Ø§Ø¨Ù‚Ø§Øª Ø§Ø³Ø¨']
            },
            {
                'name': 'ÙØ¯Ø±Ø§Ø³ÛŒÙˆÙ† Ø³ÙˆØ§Ø±Ú©Ø§Ø±ÛŒ',
                'base_url': 'https://www.iranequestrian.com',
                'search_paths': ['/news', '/articles'],
                'keywords': ['Ø§Ø³Ø¨', 'Ø³ÙˆØ§Ø±Ú©Ø§Ø±ÛŒ', 'Ù…Ø³Ø§Ø¨Ù‚Ø§Øª']
            },
            # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
        ]
        
        self.scraped_urls = set()
        self.scraped_content = []
        
    def check_robots_txt(self, base_url: str) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ robots.txt Ø¨Ø±Ø§ÛŒ Ø±Ø¹Ø§ÛŒØª Ù‚ÙˆØ§Ù†ÛŒÙ†"""
        try:
            robots_url = urljoin(base_url, '/robots.txt')
            response = self.session.get(robots_url, timeout=10)
            if response.status_code == 200:
                # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø¯Ù‡ - Ø¯Ø± production Ø¨Ø§ÛŒØ¯ Ú©Ø§Ù…Ù„â€ŒØªØ± Ø¨Ø§Ø´Ø¯
                return True
        except:
            pass
        return True  # Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… Ø¯Ø³ØªØ±Ø³ÛŒØŒ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
    
    def clean_text(self, text: str) -> str:
        """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
        if not text:
            return ""
        
        # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
        text = re.sub(r'\s+', ' ', text)
        # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ
        text = re.sub(r'[^\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFFa-zA-Z0-9\s.,!?;:()\-]', '', text)
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§
        text = text.strip()
        return text
    
    def extract_meta_tags(self, soup: BeautifulSoup) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Meta Tags Ø¨Ø±Ø§ÛŒ SEO"""
        meta_data = {
            'title': '',
            'description': '',
            'keywords': '',
            'og_title': '',
            'og_description': '',
            'og_image': '',
        }
        
        # Title
        title_tag = soup.find('title')
        if title_tag:
            meta_data['title'] = self.clean_text(title_tag.get_text())
        
        # Meta Description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            meta_data['description'] = self.clean_text(meta_desc.get('content', ''))
        
        # Meta Keywords
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords:
            meta_data['keywords'] = self.clean_text(meta_keywords.get('content', ''))
        
        # Open Graph
        og_title = soup.find('meta', attrs={'property': 'og:title'})
        if og_title:
            meta_data['og_title'] = self.clean_text(og_title.get('content', ''))
        
        og_desc = soup.find('meta', attrs={'property': 'og:description'})
        if og_desc:
            meta_data['og_description'] = self.clean_text(og_desc.get('content', ''))
        
        og_image = soup.find('meta', attrs={'property': 'og:image'})
        if og_image:
            meta_data['og_image'] = og_image.get('content', '')
        
        return meta_data
    
    def extract_content(self, soup: BeautifulSoup) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ ØµÙØ­Ù‡"""
        content = {
            'headings': [],
            'paragraphs': [],
            'images': [],
            'links': [],
        }
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ headings (H1-H6)
        for i in range(1, 7):
            headings = soup.find_all(f'h{i}')
            for heading in headings:
                text = self.clean_text(heading.get_text())
                if text:
                    content['headings'].append({
                        'level': i,
                        'text': text
                    })
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ paragraphs
        paragraphs = soup.find_all('p')
        for p in paragraphs:
            text = self.clean_text(p.get_text())
            if text and len(text) > 20:  # ÙÙ‚Ø· Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ù…Ø­ØªÙˆØ§
                content['paragraphs'].append(text)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØµØ§ÙˆÛŒØ±
        images = soup.find_all('img')
        for img in images:
            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
            if src:
                alt = img.get('alt', '')
                content['images'].append({
                    'url': src,
                    'alt': self.clean_text(alt),
                    'title': self.clean_text(img.get('title', ''))
                })
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
        links = soup.find_all('a', href=True)
        for link in links:
            href = link.get('href')
            text = self.clean_text(link.get_text())
            if href and text:
                content['links'].append({
                    'url': href,
                    'text': text
                })
        
        return content
    
    def download_image(self, image_url: str, base_url: str) -> Optional[str]:
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø°Ø®ÛŒØ±Ù‡ ØªØµÙˆÛŒØ±"""
        try:
            # ØªØ¨Ø¯ÛŒÙ„ URL Ù†Ø³Ø¨ÛŒ Ø¨Ù‡ Ù…Ø·Ù„Ù‚
            if not image_url.startswith('http'):
                image_url = urljoin(base_url, image_url)
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
            url_hash = hashlib.md5(image_url.encode()).hexdigest()
            image_ext = Path(urlparse(image_url).path).suffix or '.jpg'
            image_filename = f"{url_hash}{image_ext}"
            image_path = self.images_dir / image_filename
            
            if image_path.exists():
                return str(image_path.relative_to(self.output_dir))
            
            # Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµÙˆÛŒØ±
            response = self.session.get(image_url, timeout=30, stream=True)
            if response.status_code == 200:
                # Ø¨Ø±Ø±Ø³ÛŒ Ù†ÙˆØ¹ ÙØ§ÛŒÙ„
                content_type = response.headers.get('content-type', '')
                if 'image' not in content_type:
                    return None
                
                # Ø°Ø®ÛŒØ±Ù‡ ØªØµÙˆÛŒØ±
                image_data = response.content
                with open(image_path, 'wb') as f:
                    f.write(image_data)
                
                # Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØµÙˆÛŒØ±
                try:
                    img = Image.open(io.BytesIO(image_data))
                    # Ø§Ú¯Ø± ØªØµÙˆÛŒØ± Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ Ø§Ø³ØªØŒ resize Ú©Ù†ÛŒÙ…
                    if img.width > 1920 or img.height > 1920:
                        img.thumbnail((1920, 1920), Image.Resampling.LANCZOS)
                        img.save(image_path, optimize=True, quality=85)
                except:
                    pass
                
                return str(image_path.relative_to(self.output_dir))
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµÙˆÛŒØ± {image_url}: {e}")
        
        return None
    
    def scrape_page(self, url: str) -> Optional[Dict]:
        """Ø§Ø³Ú©Ø±Ù¾ ÛŒÚ© ØµÙØ­Ù‡"""
        if url in self.scraped_urls:
            return None
        
        try:
            print(f"Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³Ú©Ø±Ù¾: {url}")
            response = self.session.get(url, timeout=30)
            
            if response.status_code != 200:
                return None
            
            # Ø¨Ø±Ø±Ø³ÛŒ encoding
            response.encoding = response.apparent_encoding or 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Meta Tags
            meta_data = self.extract_meta_tags(soup)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§
            content = self.extract_content(soup)
            
            # Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ±
            downloaded_images = []
            for img_info in content['images'][:10]:  # Ø­Ø¯Ø§Ú©Ø«Ø± 10 ØªØµÙˆÛŒØ±
                img_path = self.download_image(img_info['url'], url)
                if img_path:
                    downloaded_images.append({
                        'path': img_path,
                        'alt': img_info['alt'],
                        'title': img_info['title']
                    })
            
            # Ø§ÛŒØ¬Ø§Ø¯ slug Ø§Ø² title
            title = meta_data['title'] or content['headings'][0]['text'] if content['headings'] else 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†'
            slug = self.create_slug(title)
            
            # ØªØ±Ú©ÛŒØ¨ Ù…Ø­ØªÙˆØ§
            full_text = ' '.join([p for p in content['paragraphs']])
            
            scraped_data = {
                'id': hashlib.md5(url.encode()).hexdigest()[:12],
                'url': url,
                'slug': slug,
                'title': title,
                'meta_description': meta_data['description'],
                'meta_keywords': meta_data['keywords'],
                'content': full_text,
                'excerpt': full_text[:300] + '...' if len(full_text) > 300 else full_text,
                'headings': content['headings'],
                'images': downloaded_images,
                'scraped_at': datetime.now().isoformat(),
                'source': urlparse(url).netloc,
            }
            
            self.scraped_urls.add(url)
            return scraped_data
            
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±Ù¾ {url}: {e}")
            return None
    
    def create_slug(self, text: str) -> str:
        """Ø§ÛŒØ¬Ø§Ø¯ slug Ø§Ø² Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù„Ø§ØªÛŒÙ† Ø¨Ø±Ø§ÛŒ slug
        persian_to_latin = {
            'Ø§': 'a', 'Ø¨': 'b', 'Ù¾': 'p', 'Øª': 't', 'Ø«': 's',
            'Ø¬': 'j', 'Ú†': 'ch', 'Ø­': 'h', 'Ø®': 'kh', 'Ø¯': 'd',
            'Ø°': 'z', 'Ø±': 'r', 'Ø²': 'z', 'Ú˜': 'zh', 'Ø³': 's',
            'Ø´': 'sh', 'Øµ': 's', 'Ø¶': 'z', 'Ø·': 't', 'Ø¸': 'z',
            'Ø¹': 'a', 'Øº': 'gh', 'Ù': 'f', 'Ù‚': 'gh', 'Ú©': 'k',
            'Ú¯': 'g', 'Ù„': 'l', 'Ù…': 'm', 'Ù†': 'n', 'Ùˆ': 'v',
            'Ù‡': 'h', 'ÛŒ': 'y', ' ': '-'
        }
        
        slug = text.lower()
        for persian, latin in persian_to_latin.items():
            slug = slug.replace(persian, latin)
        
        # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¬Ø§Ø²
        slug = re.sub(r'[^a-z0-9\-]', '', slug)
        slug = re.sub(r'-+', '-', slug)
        slug = slug.strip('-')
        
        return slug[:100]  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„
    
    def find_article_urls(self, base_url: str, search_paths: List[str], keywords: List[str]) -> List[str]:
        """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† URL Ù‡Ø§ÛŒ Ù…Ù‚Ø§Ù„Ø§Øª"""
        article_urls = []
        
        for path in search_paths:
            try:
                url = urljoin(base_url, path)
                response = self.session.get(url, timeout=30)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ù‚Ø§Ù„Ø§Øª
                    links = soup.find_all('a', href=True)
                    for link in links:
                        href = link.get('href')
                        text = link.get_text().lower()
                        
                        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ù„ÛŒÙ†Ú© Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø§Ø³Ø¨ Ø§Ø³Øª
                        if any(keyword in text for keyword in keywords):
                            full_url = urljoin(base_url, href)
                            if full_url not in article_urls:
                                article_urls.append(full_url)
                
                time.sleep(self.delay)  # ØªØ§Ø®ÛŒØ± Ø¨Ø±Ø§ÛŒ Ø±Ø¹Ø§ÛŒØª Ø§Ø®Ù„Ø§Ù‚ÛŒ
                
            except Exception as e:
                print(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ø² {path}: {e}")
        
        return article_urls[:20]  # Ø­Ø¯Ø§Ú©Ø«Ø± 20 Ù…Ù‚Ø§Ù„Ù‡ Ø§Ø² Ù‡Ø± Ø³Ø§ÛŒØª
    
    def scrape_site(self, site_config: Dict):
        """Ø§Ø³Ú©Ø±Ù¾ ÛŒÚ© Ø³Ø§ÛŒØª Ú©Ø§Ù…Ù„"""
        print(f"\n{'='*60}")
        print(f"Ø´Ø±ÙˆØ¹ Ø§Ø³Ú©Ø±Ù¾ Ø³Ø§ÛŒØª: {site_config['name']}")
        print(f"URL: {site_config['base_url']}")
        print(f"{'='*60}\n")
        
        # Ø¨Ø±Ø±Ø³ÛŒ robots.txt
        if not self.check_robots_txt(site_config['base_url']):
            print(f"âš ï¸  robots.txt Ø§Ø¬Ø§Ø²Ù‡ Ø§Ø³Ú©Ø±Ù¾ Ù†Ù…ÛŒâ€ŒØ¯Ù‡Ø¯: {site_config['base_url']}")
            return
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† URL Ù‡Ø§ÛŒ Ù…Ù‚Ø§Ù„Ø§Øª
        article_urls = self.find_article_urls(
            site_config['base_url'],
            site_config['search_paths'],
            site_config['keywords']
        )
        
        print(f"ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡: {len(article_urls)}")
        
        # Ø§Ø³Ú©Ø±Ù¾ Ù‡Ø± Ù…Ù‚Ø§Ù„Ù‡
        for url in article_urls:
            data = self.scrape_page(url)
            if data:
                self.scraped_content.append(data)
                print(f"âœ“ Ù…Ø­ØªÙˆØ§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {data['title'][:50]}...")
            
            time.sleep(self.delay)  # ØªØ§Ø®ÛŒØ± Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§
    
    def save_to_json(self, filename: str = 'scraped_content.json'):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ JSON"""
        output_file = self.data_dir / filename
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.scraped_content, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ“ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± {output_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")
        print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù…Ø­ØªÙˆØ§Ù‡Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡: {len(self.scraped_content)}")
    
    def save_to_sql(self, filename: str = 'scraped_content.sql'):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ SQL Ø¨Ø±Ø§ÛŒ import Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
        output_file = self.data_dir / filename
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("-- Ù…Ø­ØªÙˆØ§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡ Ø§Ø² Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ\n")
            f.write("-- ØªØ§Ø±ÛŒØ® ØªÙˆÙ„ÛŒØ¯: " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n\n")
            
            for item in self.scraped_content:
                # Escape Ø¨Ø±Ø§ÛŒ SQL
                title = item['title'].replace("'", "''")
                content = item['content'].replace("'", "''")
                excerpt = item['excerpt'].replace("'", "''")
                meta_desc = item['meta_description'].replace("'", "''")
                
                # ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ
                main_image = item['images'][0]['path'] if item['images'] else None
                
                f.write(f"""
INSERT INTO blog_posts (
    title, slug, excerpt, content, featured_image,
    meta_description, meta_keywords, author_id, category_id,
    is_published, published_at, created_at
) VALUES (
    '{title}',
    '{item['slug']}',
    '{excerpt}',
    '{content}',
    {f"'{main_image}'" if main_image else 'NULL'},
    '{meta_desc}',
    '{item['meta_keywords']}',
    1, -- author_id (Ø¨Ø§ÛŒØ¯ ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯)
    1, -- category_id (Ø¨Ø§ÛŒØ¯ ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯)
    true,
    NOW(),
    NOW()
);
""")
        
        print(f"\nâœ“ ÙØ§ÛŒÙ„ SQL Ø¯Ø± {output_file} Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
    
    def run(self):
        """Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±Ù¾Ø±"""
        print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…Ø­ØªÙˆØ§...")
        print(f"ğŸ“ Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ: {self.output_dir}\n")
        
        for site in self.target_sites:
            try:
                self.scrape_site(site)
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±Ù¾ Ø³Ø§ÛŒØª {site['name']}: {e}")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        if self.scraped_content:
            self.save_to_json()
            self.save_to_sql()
            
            # Ø®Ù„Ø§ØµÙ‡
            print(f"\n{'='*60}")
            print("âœ… Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…Ø­ØªÙˆØ§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")
            print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù…Ø­ØªÙˆØ§Ù‡Ø§: {len(self.scraped_content)}")
            print(f"ğŸ–¼ï¸  ØªØ¹Ø¯Ø§Ø¯ ØªØµØ§ÙˆÛŒØ± Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡: {len(list(self.images_dir.glob('*')))}")
            print(f"{'='*60}\n")
        else:
            print("âš ï¸  Ù‡ÛŒÚ† Ù…Ø­ØªÙˆØ§ÛŒÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù†Ø´Ø¯!")


def main():
    scraper = ContentScraper(output_dir="scraped_content")
    scraper.run()


if __name__ == "__main__":
    main()

