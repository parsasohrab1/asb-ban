#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Content Scraper
"""

from content_scraper import ContentScraper
import json

def example_basic_usage():
    """Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù¾Ø§ÛŒÙ‡"""
    print("=" * 60)
    print("Ù…Ø«Ø§Ù„ 1: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù¾Ø§ÛŒÙ‡")
    print("=" * 60)
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø³Ú©Ø±Ù¾Ø±
    scraper = ContentScraper(output_dir="example_output")
    
    # Ø§Ø³Ú©Ø±Ù¾ ÛŒÚ© ØµÙØ­Ù‡ Ø®Ø§Øµ
    url = "https://example.com/article-about-horses"
    data = scraper.scrape_page(url)
    
    if data:
        print(f"\nâœ“ Ù…Ø­ØªÙˆØ§ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯:")
        print(f"  Ø¹Ù†ÙˆØ§Ù†: {data['title']}")
        print(f"  ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù: {len(data.get('headings', []))}")
        print(f"  ØªØ¹Ø¯Ø§Ø¯ ØªØµØ§ÙˆÛŒØ±: {len(data.get('images', []))}")
    else:
        print("\nâŒ Ø®Ø·Ø§ Ø¯Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…Ø­ØªÙˆØ§")


def example_custom_site():
    """Ù…Ø«Ø§Ù„ Ø§Ø³Ú©Ø±Ù¾ Ø³Ø§ÛŒØª Ø³ÙØ§Ø±Ø´ÛŒ"""
    print("\n" + "=" * 60)
    print("Ù…Ø«Ø§Ù„ 2: Ø§Ø³Ú©Ø±Ù¾ Ø³Ø§ÛŒØª Ø³ÙØ§Ø±Ø´ÛŒ")
    print("=" * 60)
    
    scraper = ContentScraper(output_dir="custom_output")
    
    # ØªØ¹Ø±ÛŒÙ Ø³Ø§ÛŒØª Ø³ÙØ§Ø±Ø´ÛŒ
    custom_site = {
        'name': 'Ø³Ø§ÛŒØª Ø³ÙØ§Ø±Ø´ÛŒ',
        'base_url': 'https://example.com',
        'search_paths': ['/articles', '/blog'],
        'keywords': ['Ø§Ø³Ø¨', 'Ø³ÙˆØ§Ø±Ú©Ø§Ø±ÛŒ']
    }
    
    # Ø§Ø³Ú©Ø±Ù¾ Ø³Ø§ÛŒØª
    scraper.scrape_site(custom_site)
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    if scraper.scraped_content:
        scraper.save_to_json("custom_content.json")
        print(f"\nâœ“ {len(scraper.scraped_content)} Ù…Ø­ØªÙˆØ§ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯")


def example_filter_content():
    """Ù…Ø«Ø§Ù„ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù…Ø­ØªÙˆØ§"""
    print("\n" + "=" * 60)
    print("Ù…Ø«Ø§Ù„ 3: ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù…Ø­ØªÙˆØ§")
    print("=" * 60)
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ø­ØªÙˆØ§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡
    with open('scraped_content/data/scraped_content.json', 'r', encoding='utf-8') as f:
        contents = json.load(f)
    
    # ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
    keywords = ['Ù†Ú˜Ø§Ø¯', 'ØªØ±Ø¨ÛŒØª', 'Ø¨ÛŒÙ…Ø§Ø±ÛŒ']
    filtered = [
        c for c in contents
        if any(kw in c.get('title', '').lower() or kw in c.get('content', '').lower() 
               for kw in keywords)
    ]
    
    print(f"\nâœ“ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­ØªÙˆØ§Ù‡Ø§ÛŒ ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡: {len(filtered)}")
    for item in filtered[:5]:
        print(f"  - {item['title'][:50]}...")


def example_import_preparation():
    """Ù…Ø«Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ import"""
    print("\n" + "=" * 60)
    print("Ù…Ø«Ø§Ù„ 4: Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ import")
    print("=" * 60)
    
    from validate_content import ContentValidator
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ø­ØªÙˆØ§
    with open('scraped_content/data/scraped_content.json', 'r', encoding='utf-8') as f:
        contents = json.load(f)
    
    # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
    validator = ContentValidator()
    valid_contents, invalid_contents = validator.validate_batch(contents)
    
    print(f"\nâœ“ Ù…Ø­ØªÙˆØ§Ù‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±: {len(valid_contents)}")
    print(f"âŒ Ù…Ø­ØªÙˆØ§Ù‡Ø§ÛŒ Ù†Ø§Ù…Ø¹ØªØ¨Ø±: {len(invalid_contents)}")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø­ØªÙˆØ§Ù‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±
    if valid_contents:
        with open('scraped_content/data/validated_content.json', 'w', encoding='utf-8') as f:
            json.dump(valid_contents, f, ensure_ascii=False, indent=2)
        print("âœ“ Ù…Ø­ØªÙˆØ§Ù‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")


if __name__ == "__main__":
    print("ğŸ“š Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Content Scraper\n")
    
    # Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ (comment/uncomment Ú©Ù†ÛŒØ¯)
    # example_basic_usage()
    # example_custom_site()
    # example_filter_content()
    # example_import_preparation()
    
    print("\nğŸ’¡ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ØŒ comment Ù‡Ø§ Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø±ÛŒØ¯")

